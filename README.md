# RISC-V Assembler

A RISC-V Assembler written in Rust for educational purposes.

# Tokenizer

Tokenizes a sequence of characters (String) using 2 states recognized using regular expressions:
1. Any number of TOKEN regexes
2. One IGNORE regex

Outputs a Token sequence as a Vector. Token struct contains the original token string slice and also a TokenTypeId representing the Token type. The TokenTypeId corresponds to the index of the Token pattern given to the Tokenizer constructor.

TOKENS may be SINGLE or SURROUND types
1. SINGLE: captures the whole pattern at once
2. SURROUND: finds the begin and end pattern, and capturing them and everything in between, with optional escape characters

General rules are:
1. TOKENS can appear consecutively, or with IGNORE chars in between them. IGNORE chars are discarded.
2. Whitespace is trimmed from start and end of the input string automatically
3. Regex patterns are tried in the order given during the construction of the Tokenizer (IGNORE is always tried last)

An EOF Token is automatically added to the end of the Token sequence. This EOF Token has a TokenTypeId of -1. The TokenTypeIds are also used subsequently in parsing the token sequence.

The given regexes must be able to handle all chars in the input to tokenize successfully (except for inner characters of SURROUND tokens). For well-defined behavior, the IGNORE regex should be defined to consume all chars in between valid tokens.

To do:
> Configure trimming of whitespace


# Parser/Grammar

Table-driven Parser that creates an abstract syntax tree from a sequence of tokens (such as one created by Tokenizer). The grammar rules can be customized prior to parsing by creating a custom Grammar instance.

Currently only accepts LL(k) unambiguous grammars for any k.

Grammar is generated by configuring a GrammarGenerator, and calling generate() to return the Grammar. The Grammar should be treated immutable once generated.

GrammarGenerator is configured by creating NodeDefs of Non-Terminals and Terminals through new_nonterm or new_term, and then creating Productions using make_prod. Non-terminals must have at least 1 production. Terminals need a Regex pattern that must match any valid token for it.

Note: The parser treats all input tokens as part of the grammar, so if using the token sequence created by Tokenizer, make sure to include the EOF token in the Grammar definition.

Example LL grammar (with common prefix)

Program -> Expression EOF
Expression -> Term Operator Expression
Expression -> Term
Term -> [0-9]+
Operator -> [-+/*]

Common prefixes can be eliminated to minimize k for LL(k). This can be done by the following:

LL grammar eliminating common prefixes

Program -> Expression EOF
Expression -> Term Expression_Tail
Expression_Tail -> Operator Term Expression_Tail
Expression_Tail -> eps
Term -> [0-9]+
Operator -> [-+/*]

The first grammar is LL(2), while the second is LL(1).

To Do:
> Automatic conversion between grammar classes
> Configure resolving of ambiguity
> Descriptive errors